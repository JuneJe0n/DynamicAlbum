{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Grounding%20DINO/GroundingDINO_with_Segment_Anything.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgj3tUoobAlj"
      },
      "source": [
        "# Combining Grounding DINO with Segment Anything (SAM) for text-based mask generation\n",
        "\n",
        "In this notebook, we're going to combine 2 very cool models - [Grounding DINO](https://huggingface.co/docs/transformers/main/en/model_doc/grounding-dino) and [SAM](https://huggingface.co/docs/transformers/en/model_doc/sam). We'll use Grounding DINO to generate bounding boxes based on text prompts, after which we can prompt SAM to generate corresponding segmentation masks for them.\n",
        "\n",
        "This is based on the popular [Grounded Segment Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything) project - just with fewer lines of code as the models are now available in the Transformers library. Refer to the [paper](https://arxiv.org/abs/2401.14159) for details.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/grounded_sam.png\"\n",
        "alt=\"drawing\" width=\"900\"/>\n",
        "\n",
        "<small> Grounded SAM overview. Taken from the <a href=\"https://github.com/IDEA-Research/Grounded-Segment-Anything\">original repository</a>. </small>\n",
        "\n",
        "Author of this notebook: [Eduardo Pacheco](https://huggingface.co/EduardoPacheco) - give him a follow on Hugging\n",
        " Face!\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "Let's start by installing ðŸ¤— Transformers from source since Grounding DINO is brand new at the time of writing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78i_LKhJYz8M",
        "outputId": "c3e2387a-e82d-4fd7-8bd9-5d4b90e829d6"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade -q git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r9dNrDHy2tA"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Let's start by importing the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Y2vA9eeacmIc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, List, Dict, Optional, Union, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from transformers import AutoModelForMaskGeneration, AutoProcessor, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1NxJzCNrnjH"
      },
      "source": [
        "## Result Utils\n",
        "\n",
        "We'll store the detection results of Grounding DINO in a dedicated Python dataclass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZgeXiUwIrpqJ"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class BoundingBox:\n",
        "    xmin: int\n",
        "    ymin: int\n",
        "    xmax: int\n",
        "    ymax: int\n",
        "\n",
        "    @property\n",
        "    def xyxy(self) -> List[float]:\n",
        "        return [self.xmin, self.ymin, self.xmax, self.ymax]\n",
        "\n",
        "@dataclass\n",
        "class DetectionResult:\n",
        "    score: float\n",
        "    label: str\n",
        "    box: BoundingBox\n",
        "    mask: Optional[np.array] = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, detection_dict: Dict) -> 'DetectionResult':\n",
        "        return cls(score=detection_dict['score'],\n",
        "                   label=detection_dict['label'],\n",
        "                   box=BoundingBox(xmin=detection_dict['box']['xmin'],\n",
        "                                   ymin=detection_dict['box']['ymin'],\n",
        "                                   xmax=detection_dict['box']['xmax'],\n",
        "                                   ymax=detection_dict['box']['ymax']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCzSUQL5lAvE"
      },
      "source": [
        "## Plot Utils\n",
        "\n",
        "Below, some utility functions are defined as we'll draw the detection results of Grounding DINO on top of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Zah3Esewo4P6"
      },
      "outputs": [],
      "source": [
        "def annotate(image: Union[Image.Image, np.ndarray], detection_results: List[DetectionResult]) -> np.ndarray:\n",
        "    # Convert PIL Image to OpenCV format\n",
        "    image_cv2 = np.array(image) if isinstance(image, Image.Image) else image\n",
        "    image_cv2 = cv2.cvtColor(image_cv2, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Iterate over detections and add bounding boxes and masks\n",
        "    for detection in detection_results:\n",
        "        label = detection.label\n",
        "        score = detection.score\n",
        "        box = detection.box\n",
        "        mask = detection.mask\n",
        "\n",
        "        # Sample a random color for each detection\n",
        "        color = np.random.randint(0, 256, size=3)\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(image_cv2, (box.xmin, box.ymin), (box.xmax, box.ymax), color.tolist(), 2)\n",
        "        cv2.putText(image_cv2, f'{label}: {score:.2f}', (box.xmin, box.ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color.tolist(), 2)\n",
        "\n",
        "        # If mask is available, apply it\n",
        "        if mask is not None:\n",
        "            # Convert mask to uint8\n",
        "            mask_uint8 = (mask * 255).astype(np.uint8)\n",
        "            contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            cv2.drawContours(image_cv2, contours, -1, color.tolist(), 2)\n",
        "\n",
        "    return cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def plot_detections(\n",
        "    image: Union[Image.Image, np.ndarray],\n",
        "    detections: List[DetectionResult],\n",
        "    save_name: Optional[str] = None\n",
        ") -> None:\n",
        "    annotated_image = annotate(image, detections)\n",
        "    plt.imshow(annotated_image)\n",
        "    plt.axis('off')\n",
        "    if save_name:\n",
        "        plt.savefig(save_name, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Wzjx3MLPjyW8"
      },
      "outputs": [],
      "source": [
        "def random_named_css_colors(num_colors: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Returns a list of randomly selected named CSS colors.\n",
        "\n",
        "    Args:\n",
        "    - num_colors (int): Number of random colors to generate.\n",
        "\n",
        "    Returns:\n",
        "    - list: List of randomly selected named CSS colors.\n",
        "    \"\"\"\n",
        "    # List of named CSS colors\n",
        "    named_css_colors = [\n",
        "        'aliceblue', 'antiquewhite', 'aqua', 'aquamarine', 'azure', 'beige', 'bisque', 'black', 'blanchedalmond',\n",
        "        'blue', 'blueviolet', 'brown', 'burlywood', 'cadetblue', 'chartreuse', 'chocolate', 'coral', 'cornflowerblue',\n",
        "        'cornsilk', 'crimson', 'cyan', 'darkblue', 'darkcyan', 'darkgoldenrod', 'darkgray', 'darkgreen', 'darkgrey',\n",
        "        'darkkhaki', 'darkmagenta', 'darkolivegreen', 'darkorange', 'darkorchid', 'darkred', 'darksalmon', 'darkseagreen',\n",
        "        'darkslateblue', 'darkslategray', 'darkslategrey', 'darkturquoise', 'darkviolet', 'deeppink', 'deepskyblue',\n",
        "        'dimgray', 'dimgrey', 'dodgerblue', 'firebrick', 'floralwhite', 'forestgreen', 'fuchsia', 'gainsboro', 'ghostwhite',\n",
        "        'gold', 'goldenrod', 'gray', 'green', 'greenyellow', 'grey', 'honeydew', 'hotpink', 'indianred', 'indigo', 'ivory',\n",
        "        'khaki', 'lavender', 'lavenderblush', 'lawngreen', 'lemonchiffon', 'lightblue', 'lightcoral', 'lightcyan', 'lightgoldenrodyellow',\n",
        "        'lightgray', 'lightgreen', 'lightgrey', 'lightpink', 'lightsalmon', 'lightseagreen', 'lightskyblue', 'lightslategray',\n",
        "        'lightslategrey', 'lightsteelblue', 'lightyellow', 'lime', 'limegreen', 'linen', 'magenta', 'maroon', 'mediumaquamarine',\n",
        "        'mediumblue', 'mediumorchid', 'mediumpurple', 'mediumseagreen', 'mediumslateblue', 'mediumspringgreen', 'mediumturquoise',\n",
        "        'mediumvioletred', 'midnightblue', 'mintcream', 'mistyrose', 'moccasin', 'navajowhite', 'navy', 'oldlace', 'olive',\n",
        "        'olivedrab', 'orange', 'orangered', 'orchid', 'palegoldenrod', 'palegreen', 'paleturquoise', 'palevioletred', 'papayawhip',\n",
        "        'peachpuff', 'peru', 'pink', 'plum', 'powderblue', 'purple', 'rebeccapurple', 'red', 'rosybrown', 'royalblue', 'saddlebrown',\n",
        "        'salmon', 'sandybrown', 'seagreen', 'seashell', 'sienna', 'silver', 'skyblue', 'slateblue', 'slategray', 'slategrey',\n",
        "        'snow', 'springgreen', 'steelblue', 'tan', 'teal', 'thistle', 'tomato', 'turquoise', 'violet', 'wheat', 'white',\n",
        "        'whitesmoke', 'yellow', 'yellowgreen'\n",
        "    ]\n",
        "\n",
        "    # Sample random named CSS colors\n",
        "    return random.sample(named_css_colors, min(num_colors, len(named_css_colors)))\n",
        "\n",
        "def plot_detections_plotly(\n",
        "    image: np.ndarray,\n",
        "    detections: List[DetectionResult],\n",
        "    class_colors: Optional[Dict[str, str]] = None\n",
        ") -> None:\n",
        "    # If class_colors is not provided, generate random colors for each class\n",
        "    if class_colors is None:\n",
        "        num_detections = len(detections)\n",
        "        colors = random_named_css_colors(num_detections)\n",
        "        class_colors = {}\n",
        "        for i in range(num_detections):\n",
        "            class_colors[i] = colors[i]\n",
        "\n",
        "\n",
        "    fig = px.imshow(image)\n",
        "\n",
        "    # Add bounding boxes\n",
        "    shapes = []\n",
        "    annotations = []\n",
        "    for idx, detection in enumerate(detections):\n",
        "        label = detection.label\n",
        "        box = detection.box\n",
        "        score = detection.score\n",
        "        mask = detection.mask\n",
        "\n",
        "        polygon = mask_to_polygon(mask)\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=[point[0] for point in polygon] + [polygon[0][0]],\n",
        "            y=[point[1] for point in polygon] + [polygon[0][1]],\n",
        "            mode='lines',\n",
        "            line=dict(color=class_colors[idx], width=2),\n",
        "            fill='toself',\n",
        "            name=f\"{label}: {score:.2f}\"\n",
        "        ))\n",
        "\n",
        "        xmin, ymin, xmax, ymax = box.xyxy\n",
        "        shape = [\n",
        "            dict(\n",
        "                type=\"rect\",\n",
        "                xref=\"x\", yref=\"y\",\n",
        "                x0=xmin, y0=ymin,\n",
        "                x1=xmax, y1=ymax,\n",
        "                line=dict(color=class_colors[idx])\n",
        "            )\n",
        "        ]\n",
        "        annotation = [\n",
        "            dict(\n",
        "                x=(xmin+xmax) // 2, y=(ymin+ymax) // 2,\n",
        "                xref=\"x\", yref=\"y\",\n",
        "                text=f\"{label}: {score:.2f}\",\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        shapes.append(shape)\n",
        "        annotations.append(annotation)\n",
        "\n",
        "    # Update layout\n",
        "    button_shapes = [dict(label=\"None\",method=\"relayout\",args=[\"shapes\", []])]\n",
        "    button_shapes = button_shapes + [\n",
        "        dict(label=f\"Detection {idx+1}\",method=\"relayout\",args=[\"shapes\", shape]) for idx, shape in enumerate(shapes)\n",
        "    ]\n",
        "    button_shapes = button_shapes + [dict(label=\"All\", method=\"relayout\", args=[\"shapes\", sum(shapes, [])])]\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis=dict(visible=False),\n",
        "        yaxis=dict(visible=False),\n",
        "        # margin=dict(l=0, r=0, t=0, b=0),\n",
        "        showlegend=True,\n",
        "        updatemenus=[\n",
        "            dict(\n",
        "                type=\"buttons\",\n",
        "                direction=\"up\",\n",
        "                buttons=button_shapes\n",
        "            )\n",
        "        ],\n",
        "        legend=dict(\n",
        "            orientation=\"h\",\n",
        "            yanchor=\"bottom\",\n",
        "            y=1.02,\n",
        "            xanchor=\"right\",\n",
        "            x=1\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Show plot\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A856bC-Nha45"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vIqTxb5LhcjU"
      },
      "outputs": [],
      "source": [
        "def mask_to_polygon(mask: np.ndarray) -> List[List[int]]:\n",
        "    # Find contours in the binary mask\n",
        "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Find the contour with the largest area\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Extract the vertices of the contour\n",
        "    polygon = largest_contour.reshape(-1, 2).tolist()\n",
        "\n",
        "    return polygon\n",
        "\n",
        "def polygon_to_mask(polygon: List[Tuple[int, int]], image_shape: Tuple[int, int]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert a polygon to a segmentation mask.\n",
        "\n",
        "    Args:\n",
        "    - polygon (list): List of (x, y) coordinates representing the vertices of the polygon.\n",
        "    - image_shape (tuple): Shape of the image (height, width) for the mask.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: Segmentation mask with the polygon filled.\n",
        "    \"\"\"\n",
        "    # Create an empty mask\n",
        "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
        "\n",
        "    # Convert polygon to an array of points\n",
        "    pts = np.array(polygon, dtype=np.int32)\n",
        "\n",
        "    # Fill the polygon with white color (255)\n",
        "    cv2.fillPoly(mask, [pts], color=(255,))\n",
        "\n",
        "    return mask\n",
        "\n",
        "def load_image(image_str: str) -> Image.Image:\n",
        "    if image_str.startswith(\"http\"):\n",
        "        image = Image.open(requests.get(image_str, stream=True).raw).convert(\"RGB\")\n",
        "    else:\n",
        "        image = Image.open(image_str).convert(\"RGB\")\n",
        "\n",
        "    return image\n",
        "\n",
        "def get_boxes(results: DetectionResult) -> List[List[List[float]]]:\n",
        "    boxes = []\n",
        "    for result in results:\n",
        "        xyxy = result.box.xyxy\n",
        "        boxes.append(xyxy)\n",
        "\n",
        "    return [boxes]\n",
        "\n",
        "def refine_masks(masks: torch.BoolTensor, polygon_refinement: bool = False) -> List[np.ndarray]:\n",
        "    masks = masks.cpu().float()\n",
        "    masks = masks.permute(0, 2, 3, 1)\n",
        "    masks = masks.mean(axis=-1)\n",
        "    masks = (masks > 0).int()\n",
        "    masks = masks.numpy().astype(np.uint8)\n",
        "    masks = list(masks)\n",
        "\n",
        "    if polygon_refinement:\n",
        "        for idx, mask in enumerate(masks):\n",
        "            shape = mask.shape\n",
        "            polygon = mask_to_polygon(mask)\n",
        "            mask = polygon_to_mask(polygon, shape)\n",
        "            masks[idx] = mask\n",
        "\n",
        "    return masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fErkFJkmlEMl"
      },
      "source": [
        "## Grounded Segment Anything (SAM)\n",
        "\n",
        "Now it's time to define the Grounded SAM approach!\n",
        "\n",
        "The approach is very simple:\n",
        "1. use Grounding DINO to detect a given set of texts in the image. The output is a set of bounding boxes.\n",
        "2. prompt Segment Anything (SAM) with the bounding boxes, for which the model will output segmentation masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "8YbAUF2ZkL7Y"
      },
      "outputs": [],
      "source": [
        "def detect(\n",
        "    image: Image.Image,\n",
        "    labels: List[str],\n",
        "    threshold: float = 0.3,\n",
        "    detector_id: Optional[str] = None\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Use Grounding DINO to detect a set of labels in an image in a zero-shot fashion.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    detector_id = detector_id if detector_id is not None else \"IDEA-Research/grounding-dino-tiny\"\n",
        "    object_detector = pipeline(model=detector_id, task=\"zero-shot-object-detection\", device=device)\n",
        "\n",
        "    labels = [label if label.endswith(\".\") else label+\".\" for label in labels]\n",
        "\n",
        "    results = object_detector(image,  candidate_labels=labels, threshold=threshold)\n",
        "    results = [DetectionResult.from_dict(result) for result in results]\n",
        "\n",
        "    return results\n",
        "\n",
        "def create_mask(masks, save_dir='Grounded-Segment-Anything/data/mask', filename=None):\n",
        "    '''\n",
        "    Create each segmented mask\n",
        "    '''\n",
        "    if isinstance(masks, list):\n",
        "        combined_mask = np.zeros_like(masks[0], dtype=bool)\n",
        "        for mask in masks:\n",
        "            combined_mask = np.logical_or(combined_mask, mask)\n",
        "    else:\n",
        "        combined_mask = np.any(masks, axis=0) if masks.ndim > 2 else masks\n",
        "    \n",
        "    mask_uint8 = combined_mask.astype(np.uint8)\n",
        "    mask_image = mask_uint8 * 255  # 0->0(ê²€ì •), 1->255(í°ìƒ‰)\n",
        "    \n",
        "    mask_pil = Image.fromarray(mask_image)\n",
        "    mask_pil.save(f\"{save_dir}/{filename}.png\")\n",
        "    \n",
        "    return mask_uint8\n",
        "    \n",
        "def object_transparent_bg(original_image, mask, save_dir='Grounded-Segment-Anything/data/object', filename=None):\n",
        "    '''\n",
        "    Leave only the object, making the background be transparent\n",
        "    '''\n",
        "    if isinstance(original_image, np.ndarray):\n",
        "        if original_image.shape[2] == 3:  # RGB\n",
        "            image_pil = Image.fromarray(original_image, mode='RGB')\n",
        "        else:  # RGBA\n",
        "            image_pil = Image.fromarray(original_image, mode='RGBA')\n",
        "    else:\n",
        "        image_pil = original_image\n",
        "    \n",
        "    image_rgba = image_pil.convert('RGBA')\n",
        "    image_array = np.array(image_rgba)\n",
        "    \n",
        "    alpha = np.where(mask > 0, 255, 0).astype(np.uint8)\n",
        "    image_array[:, :, 3] = alpha\n",
        "    \n",
        "    result_image = Image.fromarray(image_array, mode='RGBA')\n",
        "    result_image.save(f\"{save_dir}/{filename}.png\")\n",
        "    \n",
        "    return result_image\n",
        "    \n",
        "\n",
        "def transparent_bg(original_image, mask, save_dir=\"Grounded-Segment-Anything/data/bg\", filename=None):\n",
        "    if isinstance(original_image, Image.Image):\n",
        "        image_array = np.array(original_image)\n",
        "    else:\n",
        "        image_array = original_image.copy()\n",
        "    \n",
        "    if len(image_array.shape) == 3:  # RGB ì´ë¯¸ì§€\n",
        "        image_array[mask > 0] = [0, 0, 0]  # ê°ì²´ ë¶€ë¶„ì„ ê²€ì •ìƒ‰ìœ¼ë¡œ\n",
        "    else:  # Grayscale\n",
        "        image_array[mask > 0] = 0\n",
        "    \n",
        "    result_image = Image.fromarray(image_array)\n",
        "    result_image.save(f\"{save_dir}/{filename}.png\")\n",
        "    \n",
        "    return result_image\n",
        "\n",
        "def segment(\n",
        "    image: Image.Image,\n",
        "    image_path: Optional[str] = None, \n",
        "    detection_results: List[Dict[str, Any]] = None,\n",
        "    polygon_refinement: bool = False,\n",
        "    segmenter_id: Optional[str] = None\n",
        ") -> List[DetectionResult]:\n",
        "    \"\"\"\n",
        "    Use Segment Anything (SAM) to generate masks given an image + a set of bounding boxes.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    segmenter_id = segmenter_id if segmenter_id is not None else \"facebook/sam-vit-base\"\n",
        "\n",
        "    segmentator = AutoModelForMaskGeneration.from_pretrained(segmenter_id).to(device)\n",
        "    processor = AutoProcessor.from_pretrained(segmenter_id)\n",
        "\n",
        "    boxes = get_boxes(detection_results)\n",
        "    inputs = processor(images=image, input_boxes=boxes, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    outputs = segmentator(**inputs)\n",
        "    masks = processor.post_process_masks(\n",
        "        masks=outputs.pred_masks,\n",
        "        original_sizes=inputs.original_sizes,\n",
        "        reshaped_input_sizes=inputs.reshaped_input_sizes\n",
        "    )[0]\n",
        "\n",
        "    fname = image_path.split('/')[-1]\n",
        "    fname = fname.split('.')[0]\n",
        "    print(f\"fname: {fname}\")\n",
        "    masks = refine_masks(masks, polygon_refinement) # list\n",
        "    mask_np = create_mask(masks, filename=fname)\n",
        "\n",
        "    arr_image = np.array(image)\n",
        "    object_transparent_bg(arr_image, mask_np, filename=fname)\n",
        "    transparent_bg(arr_image, mask_np, filename=fname)\n",
        "\n",
        "    for detection_result, mask in zip(detection_results, masks):\n",
        "        detection_result.mask = mask\n",
        "\n",
        "    return detection_results\n",
        "\n",
        "def grounded_segmentation(\n",
        "    image_path: Union[Image.Image, str],\n",
        "    labels: List[str],\n",
        "    threshold: float = 0.3,\n",
        "    polygon_refinement: bool = False,\n",
        "    detector_id: Optional[str] = None,\n",
        "    segmenter_id: Optional[str] = None\n",
        ") -> Tuple[np.ndarray, List[DetectionResult]]:\n",
        "    if isinstance(image_path, str):\n",
        "        image = load_image(image_path)\n",
        "\n",
        "    detections = detect(image, labels, threshold, detector_id)\n",
        "    detections = segment(image, image_path, detections, polygon_refinement, segmenter_id)\n",
        "\n",
        "    return np.array(image), detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo8cGKdxXWPR"
      },
      "source": [
        "### Inference\n",
        "\n",
        "Let's showcase Grounded SAM on our favorite image: the cats image from the COCO dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ypK7YEMQXXnA"
      },
      "outputs": [],
      "source": [
        "FOLDER_PATH = 'Grounded-Segment-Anything/data/album'\n",
        "threshold = 0.3\n",
        "\n",
        "detector_id = \"IDEA-Research/grounding-dino-tiny\"\n",
        "segmenter_id = \"facebook/sam2-hiera-large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "glgYnsahXYTw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30174.85it/s]\n",
            "Device set to use cpu\n",
            "Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17225.07it/s]\n",
            "Fetching 3 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 34192.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fname: 4 ONLY\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/0z/j2n4whp57zxgjc1rwnh6p26w0000gn/T/ipykernel_20888/2199632870.py:46: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  image_pil = Image.fromarray(original_image, mode='RGB')\n",
            "/var/folders/0z/j2n4whp57zxgjc1rwnh6p26w0000gn/T/ipykernel_20888/2199632870.py:58: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  result_image = Image.fromarray(image_array, mode='RGBA')\n",
            "Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 3088.59it/s]\n",
            "Device set to use cpu\n",
            "Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 19972.88it/s]\n",
            "Fetching 3 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 27533.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fname: BLACKPINK & Selena Gomez - Ice Cream - Single\n"
          ]
        }
      ],
      "source": [
        "folder = Path(FOLDER_PATH)\n",
        "image_files = sorted([str(f) for f in folder.iterdir() if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']])[:2] # path for each image\n",
        "text_prompts = [\n",
        "    ['circle'],\n",
        "    ['ice cream']\n",
        "]\n",
        "\n",
        "for idx, file_path in enumerate(image_files):\n",
        "    image_array, detections = grounded_segmentation(\n",
        "        image_path=file_path,\n",
        "        labels=text_prompts[idx],\n",
        "        threshold=threshold,\n",
        "        polygon_refinement=True,\n",
        "        detector_id=detector_id,\n",
        "        segmenter_id=segmenter_id\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dino",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
